{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Text analysis and feature extraction\n",
    "In order to feed text to a model we need to transform it to a numerical features, in this notebook we will discuss how to build a bag-of-words model from text to use it later for different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words:\n",
    "The \"Bag of Words\" (BoW) is a common and straightforward technique used in natural language processing (NLP) for text analysis and feature extraction. It represents a document as a collection, or \"bag,\" of individual words, disregarding grammar, word order, and context, and focusing solely on word frequency.\n",
    "\n",
    "Count the occurrences of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>fascinating</th>\n",
       "      <th>is</th>\n",
       "      <th>language</th>\n",
       "      <th>learn</th>\n",
       "      <th>love</th>\n",
       "      <th>more</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love natural language processing.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP is fascinating, and I want to learn more.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               and  fascinating  is  language  \\\n",
       "I love natural language processing.              0            0   0         1   \n",
       "NLP is fascinating, and I want to learn more.    1            1   1         0   \n",
       "\n",
       "                                               learn  love  more  natural  \\\n",
       "I love natural language processing.                0     1     0        1   \n",
       "NLP is fascinating, and I want to learn more.      1     0     1        0   \n",
       "\n",
       "                                               nlp  processing  to  want  \n",
       "I love natural language processing.              0           1   0     0  \n",
       "NLP is fascinating, and I want to learn more.    1           0   1     1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pandas library and alias it as 'pd'\n",
    "import pandas as pd\n",
    "\n",
    "# Import the CountVectorizer class from scikit-learn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define a list of text documents\n",
    "texts = ['I love natural language processing.', 'NLP is fascinating, and I want to learn more.']\n",
    "\n",
    "# Create an instance of the CountVectorizer class\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the text data to build the vocabulary\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Transform the text data into a document-term matrix (BoW representation)\n",
    "x = vectorizer.transform(texts)\n",
    "\n",
    "# Get the feature (word) names from the vocabulary\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a pandas DataFrame using the BoW matrix, with columns representing words and rows representing documents\n",
    "# This DataFrame visualizes the frequency of each word in each document\n",
    "pd.DataFrame(x.todense(), columns=columns, index=texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'fascinating', 'is', 'language', 'learn', 'love', 'more',\n",
       "       'natural', 'nlp', 'processing', 'to', 'want'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fascinating', 'language', 'learn', 'love', 'natural', 'nlp',\n",
       "       'processing', 'want'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop-words\n",
    "\n",
    "Stop-words are words that are not significant to the topic in hand, for example `[am, is, are, in, at, ...]` can be considered stop-words in many applications as they don't add meaning.\n",
    "\n",
    " Stop words are extremely common words that appear frequently in text but often do not carry significant meaning on their own. Examples of stop words in English include \"the,\" \"and,\" \"in,\" \"of,\" \"to,\" \"a,\" \"an,\" and so on.\n",
    "\n",
    "In some other domains and problems you may have different kind of stop-words, for example if you are processing some chatbot data you may find `[can you please, would you please, can I, may I, ...]` such examples don't add meaning so stop-words can also be domain specific, and `TFIDF` can help you find these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fascinating</th>\n",
       "      <th>language</th>\n",
       "      <th>learn</th>\n",
       "      <th>love</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love natural language processing.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP is fascinating, and I want to learn more.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fascinating  language  learn  \\\n",
       "I love natural language processing.                      0         1      0   \n",
       "NLP is fascinating, and I want to learn more.            1         0      1   \n",
       "\n",
       "                                               love  natural  nlp  processing  \\\n",
       "I love natural language processing.               1        1    0           1   \n",
       "NLP is fascinating, and I want to learn more.     0        0    1           0   \n",
       "\n",
       "                                               want  \n",
       "I love natural language processing.               0  \n",
       "NLP is fascinating, and I want to learn more.     1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list of text documents\n",
    "texts = ['I love natural language processing.', 'NLP is fascinating, and I want to learn more.']\n",
    "\n",
    "# Create an instance of the CountVectorizer class and specify that you want to remove English stop words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the text data to build the vocabulary and remove stop words\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Transform the text data into a document-term matrix (BoW representation)\n",
    "x = vectorizer.transform(texts)\n",
    "\n",
    "# Get the feature (word) names from the vocabulary\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a pandas DataFrame using the BoW matrix, with columns representing words and rows representing documents\n",
    "# This DataFrame visualizes the frequency of each remaining word in each document after stop words removal\n",
    "df = pd.DataFrame(x.todense(), columns=columns, index=texts)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the words `and`,`is`,`more`,`to` werw removed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "N-Grams is a way we can use to count for the context in the text, the bigger n-gram range the bigger context you can capture but also more features to generate, so be careful not to break your memory.\n",
    "\n",
    "In natural language processing (NLP), N-grams are contiguous sequences of N items, where the items are typically words, characters, or tokens extracted from a text or speech. N-grams are used to capture the local linguistic structure and the relationships between neighboring words in a given piece of text. \n",
    "\n",
    "Here are some common types of N-grams:\n",
    "\n",
    "* Unigrams (1-grams): These are single words in a text. For example, in the sentence \"I love NLP,\" the unigrams are [\"I,\" \"love,\" \"NLP\"].\n",
    "\n",
    "* Bigrams (2-grams): Bigrams consist of pairs of consecutive words in a text. For the same sentence, the bigrams would be [\"I love,\" \"love NLP\"].\n",
    "\n",
    "* Trigrams (3-grams): Trigrams are sequences of three consecutive words in a text. For example, \"I love NLP\" would have trigrams [\"I love NLP\"].\n",
    "\n",
    "* 4-grams, 5-grams, and so on: These refer to sequences of N consecutive words in a text, where N can be any positive integer. For example, a 4-gram for the sentence \"I love natural language processing\" would be [\"I love natural language,\" \"love natural language processing\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fascinating</th>\n",
       "      <th>fascinating want</th>\n",
       "      <th>language</th>\n",
       "      <th>language processing</th>\n",
       "      <th>learn</th>\n",
       "      <th>love</th>\n",
       "      <th>love natural</th>\n",
       "      <th>natural</th>\n",
       "      <th>natural language</th>\n",
       "      <th>nlp</th>\n",
       "      <th>nlp fascinating</th>\n",
       "      <th>processing</th>\n",
       "      <th>want</th>\n",
       "      <th>want learn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love natural language processing.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP is fascinating, and I want to learn more.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fascinating  fascinating want  \\\n",
       "I love natural language processing.                      0                 0   \n",
       "NLP is fascinating, and I want to learn more.            1                 1   \n",
       "\n",
       "                                               language  language processing  \\\n",
       "I love natural language processing.                   1                    1   \n",
       "NLP is fascinating, and I want to learn more.         0                    0   \n",
       "\n",
       "                                               learn  love  love natural  \\\n",
       "I love natural language processing.                0     1             1   \n",
       "NLP is fascinating, and I want to learn more.      1     0             0   \n",
       "\n",
       "                                               natural  natural language  nlp  \\\n",
       "I love natural language processing.                  1                 1    0   \n",
       "NLP is fascinating, and I want to learn more.        0                 0    1   \n",
       "\n",
       "                                               nlp fascinating  processing  \\\n",
       "I love natural language processing.                          0           1   \n",
       "NLP is fascinating, and I want to learn more.                1           0   \n",
       "\n",
       "                                               want  want learn  \n",
       "I love natural language processing.               0           0  \n",
       "NLP is fascinating, and I want to learn more.     1           1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list of text documents\n",
    "texts = ['I love natural language processing.', 'NLP is fascinating, and I want to learn more.']\n",
    "\n",
    "# Create an instance of the CountVectorizer class and specify:\n",
    "# - stop_words='english': Remove common English stop words\n",
    "# - ngram_range=(1, 2): Generate both unigrams and bigrams\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the text data to build the vocabulary, remove stop words, and create n-grams\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Transform the text data into a document-term matrix (BoW representation)\n",
    "x = vectorizer.transform(texts)\n",
    "\n",
    "# Get the feature (word or n-gram) names from the vocabulary\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a pandas DataFrame using the BoW matrix, with columns representing words or n-grams\n",
    "# and rows representing documents. This DataFrame visualizes the frequency of each remaining word or n-gram in each document\n",
    "pd.DataFrame(x.todense(), columns=columns, index=texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that while N-grams provide valuable context information, they can also lead to high-dimensional feature spaces, especially when dealing with a large vocabulary. In practice, N-grams are often used in combination with techniques like feature selection and dimensionality reduction to manage the complexity of text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "\n",
    "TF-IDF stands for \"Term Frequency-Inverse Document Frequency.\" It is a numerical statistic used in natural language processing (NLP) and information retrieval to evaluate the importance of a word within a document relative to a collection of documents (corpus). TF-IDF is a technique for text feature extraction that helps in quantifying the relevance of words in a document to a specific search query or task.\n",
    "\n",
    "• Instead of just counting the frequency of each word, each word here is weighted using TF-IDF\n",
    "\n",
    "• Focus on unique words in the corpus \n",
    "\n",
    "$$W_{x, y} = tf_{x, y} \\times log(\\frac{N}{df_x})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fascinating</th>\n",
       "      <th>fascinating want</th>\n",
       "      <th>language</th>\n",
       "      <th>language processing</th>\n",
       "      <th>learn</th>\n",
       "      <th>love</th>\n",
       "      <th>love natural</th>\n",
       "      <th>natural</th>\n",
       "      <th>natural language</th>\n",
       "      <th>nlp</th>\n",
       "      <th>nlp fascinating</th>\n",
       "      <th>processing</th>\n",
       "      <th>want</th>\n",
       "      <th>want learn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love natural language processing.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLP is fascinating, and I want to learn more.</th>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fascinating  fascinating want  \\\n",
       "I love natural language processing.               0.000000          0.000000   \n",
       "NLP is fascinating, and I want to learn more.     0.377964          0.377964   \n",
       "\n",
       "                                               language  language processing  \\\n",
       "I love natural language processing.            0.377964             0.377964   \n",
       "NLP is fascinating, and I want to learn more.  0.000000             0.000000   \n",
       "\n",
       "                                                  learn      love  \\\n",
       "I love natural language processing.            0.000000  0.377964   \n",
       "NLP is fascinating, and I want to learn more.  0.377964  0.000000   \n",
       "\n",
       "                                               love natural   natural  \\\n",
       "I love natural language processing.                0.377964  0.377964   \n",
       "NLP is fascinating, and I want to learn more.      0.000000  0.000000   \n",
       "\n",
       "                                               natural language       nlp  \\\n",
       "I love natural language processing.                    0.377964  0.000000   \n",
       "NLP is fascinating, and I want to learn more.          0.000000  0.377964   \n",
       "\n",
       "                                               nlp fascinating  processing  \\\n",
       "I love natural language processing.                   0.000000    0.377964   \n",
       "NLP is fascinating, and I want to learn more.         0.377964    0.000000   \n",
       "\n",
       "                                                   want  want learn  \n",
       "I love natural language processing.            0.000000    0.000000  \n",
       "NLP is fascinating, and I want to learn more.  0.377964    0.377964  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the TfidfVectorizer class from scikit-learn's feature_extraction.text module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define a list of text documents\n",
    "texts = ['I love natural language processing.', 'NLP is fascinating, and I want to learn more.']\n",
    "\n",
    "# Create an instance of the TfidfVectorizer class and specify:\n",
    "# - stop_words='english': Remove common English stop words\n",
    "# - ngram_range=(1, 2): Generate both unigrams and bigrams\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the text data to build the TF-IDF vocabulary, remove stop words, and create TF-IDF features\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Transform the text data into a TF-IDF matrix\n",
    "x = vectorizer.transform(texts)\n",
    "\n",
    "# Get the feature (word or n-gram) names from the TF-IDF vocabulary\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a pandas DataFrame using the TF-IDF matrix, with columns representing words or n-grams\n",
    "# and rows representing documents. This DataFrame visualizes the TF-IDF scores for each term in each document.\n",
    "pd.DataFrame(x.todense(), columns=columns, index=texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA : With all the recent problems the Indians have been having\n",
      "with their pitching staff I have heard numerous names\n",
      "thrown around about who could solve their problem.\n",
      "\n",
      "One name I have not heard is Mike Soper (RP).  As far as\n",
      "I know, Soper has had pretty good minor league stats.\n",
      "Why not give the kid a chance?  Anyone know anything about\n",
      "this guy?\n",
      "\n",
      "-- \n",
      "LABEL: rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from collections import Counter  # For counting elements\n",
    "import random  # For generating random values\n",
    "from termcolor import colored  # For colored console output\n",
    "from sklearn.datasets import fetch_20newsgroups  # For loading the 20 Newsgroups dataset\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances  # For cosine similarity calculations\n",
    "from sklearn.model_selection import train_test_split  # For data splitting\n",
    "\n",
    "# Load the 20 Newsgroups dataset, specifically the 'test' subset,\n",
    "# while removing 'headers', 'footers', and 'quotes'.\n",
    "# Select categories related to 'rec.autos', 'comp.windows.x', 'soc.religion.christian', and 'rec.sport.baseball'.\n",
    "data = fetch_20newsgroups(subset='test', remove=['headers', 'footers', 'quotes'],\n",
    "                         categories=['rec.autos', 'comp.windows.x', \n",
    "                                     'soc.religion.christian', 'rec.sport.baseball'])\n",
    "\n",
    "# Extract the text data (documents) and labels\n",
    "x = data.data  # Text data\n",
    "y = [data.target_names[i] for i in data.target]  # Labels\n",
    "\n",
    "# Print the first document (data) and its corresponding label\n",
    "print(f'DATA : {x[0]}')\n",
    "print(f'LABEL: {y[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'rec.sport.baseball': 397,\n",
       "         'soc.religion.christian': 398,\n",
       "         'comp.windows.x': 395,\n",
       "         'rec.autos': 396})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get the top-5 similar articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CountVectorizer class and specify that you want to remove common English stop words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training data (x_train) to build the vocabulary and remove stop words\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "# Transform the training data (x_train) into a document-term matrix (BoW representation)\n",
    "x_train_v = vectorizer.transform(x_train)\n",
    "\n",
    "# Transform the test data (x_test) into a document-term matrix (BoW representation) using the same vocabulary\n",
    "x_test_v = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 192\n",
      "True label: comp.windows.x\n",
      "0 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.214\u001b[0m\n",
      "1 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.204\u001b[0m\n",
      "2 nearest label is \u001b[91mrec.autos\u001b[0m similarity: \u001b[93m0.164\u001b[0m\n",
      "ID: 176\n",
      "True label: soc.religion.christian\n",
      "0 nearest label is \u001b[92msoc.religion.christian\u001b[0m similarity: \u001b[93m0.339\u001b[0m\n",
      "1 nearest label is \u001b[92msoc.religion.christian\u001b[0m similarity: \u001b[93m0.333\u001b[0m\n",
      "2 nearest label is \u001b[92msoc.religion.christian\u001b[0m similarity: \u001b[93m0.312\u001b[0m\n",
      "ID: 131\n",
      "True label: comp.windows.x\n",
      "0 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.577\u001b[0m\n",
      "1 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.469\u001b[0m\n",
      "2 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.469\u001b[0m\n",
      "ID: 304\n",
      "True label: comp.windows.x\n",
      "0 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.356\u001b[0m\n",
      "1 nearest label is \u001b[92mcomp.windows.x\u001b[0m similarity: \u001b[93m0.196\u001b[0m\n",
      "2 nearest label is \u001b[91mrec.autos\u001b[0m similarity: \u001b[93m0.19\u001b[0m\n",
      "ID: 52\n",
      "True label: rec.sport.baseball\n",
      "0 nearest label is \u001b[92mrec.sport.baseball\u001b[0m similarity: \u001b[93m0.925\u001b[0m\n",
      "1 nearest label is \u001b[92mrec.sport.baseball\u001b[0m similarity: \u001b[93m0.187\u001b[0m\n",
      "2 nearest label is \u001b[92mrec.sport.baseball\u001b[0m similarity: \u001b[93m0.181\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Loop through 5 random test data samples\n",
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "YELLOW = '\\033[93m'\n",
    "RESET = '\\033[0m'\n",
    "for i in random.choices(range(0, len(x_test)), k=5):\n",
    "    print(f\"ID: {i}\")\n",
    "\n",
    "    # Print the true label in green color\n",
    "    print(\"True label:\", colored(y_test[i], GREEN))\n",
    "\n",
    "    # Calculate cosine similarity between the current test data sample and all training data samples\n",
    "    distances = cosine_similarity(x_test_v[i], x_train_v).flatten()\n",
    "\n",
    "    # Sort the training data indices by similarity in descending order\n",
    "    indices = np.argsort(distances)[::-1]\n",
    "\n",
    "    # Loop through the top 3 nearest neighbors\n",
    "    for _, j in enumerate(indices[:3]):\n",
    "        # Print the neighbor's label in green if it matches the true label, or in red if it doesn't\n",
    "        print(f\"{_} nearest label is {GREEN if y_train[j] == y_test[i] else RED}{y_train[j]}{RESET}\",\n",
    "      f\"similarity: {YELLOW}{round(distances[j], 3)}{RESET}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
