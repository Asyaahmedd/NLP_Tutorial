{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenization \n",
    "Text tokenization is the process of breaking down a longer text into smaller units, known as tokens. These tokens can be words, phrases, sentences, or even individual characters, depending on the level of granularity required for the analysis.\n",
    "Tokenization is a fundamental step in natural language processing (NLP) and is used to prepare textual data for further analysis, such as text classification, sentiment analysis, language modeling, and information retrieval.\n",
    "\n",
    "Here are the common types of text tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization:\n",
    "\n",
    "This is the most common type of tokenization, where the text is split into individual words or terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'Welcome', 'to', 'NLP', 'tutorial', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import the word_tokenize function from the nltk.tokenize module\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a text string that you want to tokenize\n",
    "text = \"Hello everyone. Welcome to NLP tutorial.\"\n",
    "\n",
    "# Use the word_tokenize function to tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Print the list of tokens (words) obtained from the text\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization:\n",
    "In this case, the text is divided into individual sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'Welcome to my Github profile.', \"let's study NLP.\"]\n"
     ]
    }
   ],
   "source": [
    "# Import the sent_tokenize function from the nltk.tokenize module\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Define a text string that you want to sentence tokenize\n",
    "text = \"Hello everyone. Welcome to my Github profile. let's study NLP.\"\n",
    "\n",
    "# Use the sent_tokenize function to tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print the list of sentences obtained from the text\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrasal Tokenization:\n",
    "Sometimes, it's useful to tokenize text into multi-word phrases or n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language\n",
      "language processing\n",
      "processing is\n",
      "is a\n",
      "a subfield\n",
      "subfield of\n",
      "of artificial\n",
      "artificial intelligence\n",
      "intelligence .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing is a subfield of artificial intelligence.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Define the n-gram size (e.g., 2 for bigrams, 3 for trigrams)\n",
    "n = 2\n",
    "\n",
    "# Create a list of n-grams using NLTK's ngrams function\n",
    "ngram_list = list(ngrams(words, n))\n",
    "\n",
    "# Print the list of n-grams\n",
    "for ngram in ngram_list:\n",
    "    print(\" \".join(ngram))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenization:\n",
    "In some cases, text is tokenized into individual characters. \n",
    "Tokenizing text into individual characters is not as common as word or sentence tokenization, but it can be useful for specific tasks like Text Generation,Spelling Correction and Handwriting Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      ",\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# Tokenize the text into individual characters\n",
    "characters = list(text)\n",
    "\n",
    "# Print the list of individual characters\n",
    "for char in characters:\n",
    "    print(char)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
